{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problème de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 7.0</td>\n",
       "      <td> 0.27</td>\n",
       "      <td> 0.36</td>\n",
       "      <td> 20.7</td>\n",
       "      <td> 0.045</td>\n",
       "      <td> 45</td>\n",
       "      <td> 170</td>\n",
       "      <td> 1.0010</td>\n",
       "      <td> 3.00</td>\n",
       "      <td> 0.45</td>\n",
       "      <td>  8.8</td>\n",
       "      <td> 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 6.3</td>\n",
       "      <td> 0.30</td>\n",
       "      <td> 0.34</td>\n",
       "      <td>  1.6</td>\n",
       "      <td> 0.049</td>\n",
       "      <td> 14</td>\n",
       "      <td> 132</td>\n",
       "      <td> 0.9940</td>\n",
       "      <td> 3.30</td>\n",
       "      <td> 0.49</td>\n",
       "      <td>  9.5</td>\n",
       "      <td> 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 8.1</td>\n",
       "      <td> 0.28</td>\n",
       "      <td> 0.40</td>\n",
       "      <td>  6.9</td>\n",
       "      <td> 0.050</td>\n",
       "      <td> 30</td>\n",
       "      <td>  97</td>\n",
       "      <td> 0.9951</td>\n",
       "      <td> 3.26</td>\n",
       "      <td> 0.44</td>\n",
       "      <td> 10.1</td>\n",
       "      <td> 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 7.2</td>\n",
       "      <td> 0.23</td>\n",
       "      <td> 0.32</td>\n",
       "      <td>  8.5</td>\n",
       "      <td> 0.058</td>\n",
       "      <td> 47</td>\n",
       "      <td> 186</td>\n",
       "      <td> 0.9956</td>\n",
       "      <td> 3.19</td>\n",
       "      <td> 0.40</td>\n",
       "      <td>  9.9</td>\n",
       "      <td> 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 7.2</td>\n",
       "      <td> 0.23</td>\n",
       "      <td> 0.32</td>\n",
       "      <td>  8.5</td>\n",
       "      <td> 0.058</td>\n",
       "      <td> 47</td>\n",
       "      <td> 186</td>\n",
       "      <td> 0.9956</td>\n",
       "      <td> 3.19</td>\n",
       "      <td> 0.40</td>\n",
       "      <td>  9.9</td>\n",
       "      <td> 6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                   45                   170   1.0010  3.00       0.45   \n",
       "1                   14                   132   0.9940  3.30       0.49   \n",
       "2                   30                    97   0.9951  3.26       0.44   \n",
       "3                   47                   186   0.9956  3.19       0.40   \n",
       "4                   47                   186   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/\"\n",
    "file2 = \"winequality-white.csv\"\n",
    "import pyensae\n",
    "data = pyensae.download_data(file2, website=url2)\n",
    "import pandas\n",
    "dfwhite = pandas.read_csv(\"winequality-white.csv\",sep=\";\")\n",
    "dfwhite.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation des packages nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "import prettyplotlib as ppl\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation de la distribution des qualités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    2198\n",
       "5    1457\n",
       "7     880\n",
       "8     175\n",
       "4     163\n",
       "3      20\n",
       "9       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre d'occurrences dans la base complète de chaque qualité\n",
    "dfwhite.quality.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Histogramme\n",
    "from pylab import *\n",
    "plt.style.use('ggplot')\n",
    "pandas.DataFrame.hist(dfwhite, column=\"quality\", bins=7, color='indianred', grid=False)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Couleurs disponibles\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.colors as colors\n",
    "import math\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ratio = 1.0 / 3.0\n",
    "count = math.ceil(math.sqrt(len(colors.cnames)))\n",
    "x_count = count * ratio\n",
    "y_count = count / ratio\n",
    "x = 0\n",
    "y = 0\n",
    "w = 1 / x_count\n",
    "h = 1 / y_count\n",
    "\n",
    "for c in colors.cnames:\n",
    "    pos = (x / x_count, y / y_count)\n",
    "    ax.add_patch(patches.Rectangle(pos, w, h, color=c))\n",
    "    ax.annotate(c, xy=pos)\n",
    "    if y >= y_count-1:\n",
    "        x += 1\n",
    "        y = 0\n",
    "    else:\n",
    "        y += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Séparation de la base en une base d'entraînement et une base de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Pour que la même base de test sorte à chaque fois qu'on exécute le programme, on utilise : random_state=un nombre entier quelconque choisi arbitrairement\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfwhite[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']],dfwhite.quality, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.250102082482646"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vérification de la taille de la base de test\n",
    "len(y_test)/len(dfwhite)\n",
    "#On doit trouver 25% qui est le paramètre par défaut dans scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire simple (Chapitre 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression linéaire simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  7.08672075e-02,  -1.91050896e+00,   3.92700519e-02,\n",
       "          8.24994938e-02,  -3.62076438e-02,   3.79589490e-03,\n",
       "         -5.95091904e-04,  -1.47240724e+02,   7.30192091e-01,\n",
       "          6.56917370e-01,   1.94989276e-01]),\n",
       " 146.99034335580782,\n",
       " 'R^2=',\n",
       " 0.28213632011280609)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement du modèle sur la base d'apprentissage\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reglin = LinearRegression(normalize=True)\n",
    "#On utilise normalize=True car les variables ont des ordres de grandeurs très différents\n",
    "\n",
    "reglin.fit(X_train, y_train)\n",
    "\n",
    "reglin.coef_, reglin.intercept_, \"R^2=\",reglin.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction de la qualité sur la base de test\n",
    "predicted_reglin = reglin.predict(X_test)\n",
    "expected_reglin = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5179445350734094\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_reglin=0\n",
    "for i in range(0,len(predicted_reglin)) :\n",
    "    if round(predicted_reglin[i])==expected_reglin[i] :\n",
    "        nb_goodpred_reglin=nb_goodpred_reglin+1\n",
    "print(nb_goodpred_reglin/(len(predicted_reglin)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701.192723457\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique totale\n",
    "e_reglin_2=((predicted_reglin-expected_reglin)**2).sum()\n",
    "print(e_reglin_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "719.379822215\n",
      "0.587248834462\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue totale\n",
    "e_reglin_1=abs(predicted_reglin-expected_reglin).sum()\n",
    "print(e_reglin_1)\n",
    "#Erreur absolue moyenne\n",
    "print(e_reglin_1/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557.744327654\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs absolues inférieures à epsilon = 0.5\n",
    "e_reglin_epsilon1=sum(abs(predicted_reglin[i]-expected_reglin[i]) for i in range(0, len(y_test)) if abs(predicted_reglin[i]-expected_reglin[i])>0.5)\n",
    "print(e_reglin_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.542109538\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs absolues inférieures ou égales à epsilon = 1\n",
    "e_reglin_epsilon2=sum(abs(predicted_reglin[i]-expected_reglin[i]) for i in range(0, len(y_test)) if abs(predicted_reglin[i]-expected_reglin[i])>1)\n",
    "print(e_reglin_epsilon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Introduction de bruit et arrondi à .1 des prédictions pour mieux voir les résultats sur les graphes suivants\n",
    "\n",
    "expected_reglin_noise=expected_reglin+np.random.uniform(0,0.2,size=len(y_test))\n",
    "\n",
    "\n",
    "predicted_reglin_simplified=np.around(predicted_reglin,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin avec du bruit et arrondi à 0.1 près des prédictions : comparaison des qualités prédites et des qualités réelles\n",
    "\n",
    "plt.hexbin(expected_reglin_noise,predicted_reglin_simplified, cmap=plt.cm.YlGn)\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Qualité prédite')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.plot([4.5,7.2],[4.5,7.2],'--k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe scatter avec bruit et arrondi à 0.1 près des prédictions pour mieux voir les prédictions des qualités en faible proportion dans la base de test\n",
    "ppl.scatter(expected_reglin_noise, predicted_reglin_simplified)\n",
    "ppl.plot([3,9],[3,9],'--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Qualité prédite')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphique : Erreur absolue selon caractéristique chimique (à décliner selon la variable voulue)\n",
    "ppl.scatter(x=X_test[:,7],y=abs(expected_reglin-predicted_reglin), color='green')\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('Density')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphique : Erreur absolue selon caractéristique chimique (à décliner selon la variable voulue)\n",
    "#Avec les couleurs, on voit ainsi mieux si les pics d'erreurs sur les graphiques précédents ne sont pas tout simplement dûs au fait qu'il y a plus de vins présentant des valeurs similaires pour la variable considérée et donc il y a plus de \"chances\" que les vins tres mal prédits présentent aussi cette valeur pour la variable considérée \n",
    "ppl.scatter(x=arange(0,len(y_test)),y=abs(expected_reglin-predicted_reglin), c=X_test[:,10])\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('Alcohol')\n",
    "plt.xlabel('Vins blans')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe: erreur absolue selon la qualité espérée\n",
    "e_reglin_plot=abs(predicted_reglin-expected_reglin)\n",
    "\n",
    "ppl.scatter(expected_reglin_noise, e_reglin_plot, color='indianred')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur absolue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ANNEXE : En enlevant la variable pH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3673, 11) (3673, 10)\n"
     ]
    }
   ],
   "source": [
    "#On utilise la même base d'apprentissage et la même base de test avec pour seule différence qu'elles ne contiennent plus la colonne pH\n",
    "X_train_bis=np.concatenate((X_train[:,0:8],X_train[:,9:11]), axis=1)\n",
    "X_test_bis=np.concatenate((X_test[:,0:8],X_test[:,9:11]), axis=1)\n",
    "print(X_train.shape,X_train_bis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -2.44835323e-02,  -1.99471791e+00,  -1.29122247e-02,\n",
       "          5.11683132e-02,  -6.19574206e-01,   4.07787971e-03,\n",
       "         -6.04879330e-04,  -7.01651721e+01,   6.26933940e-01,\n",
       "          2.83162184e-01]), 72.702959038751047, 'R^2=', 0.27511073453404677)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement du modèle sur la \"nouvelle\" base d'apprentissage qui ne contient plus pH\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reglin = LinearRegression(normalize=True)\n",
    "\n",
    "reglin.fit(X_train_bis, y_train)\n",
    "\n",
    "reglin.coef_, reglin.intercept_, \"R^2=\",reglin.score(X_train_bis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction sur la base de test\n",
    "predicted_reglin_bis = reglin.predict(X_test_bis)\n",
    "expected_reglin_bis = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5154975530179445\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_reglin_bis=0\n",
    "for i in range(0,len(predicted_reglin_bis)) :\n",
    "    if round(predicted_reglin_bis[i])==expected_reglin_bis[i] :\n",
    "        nb_goodpred_reglin_bis=nb_goodpred_reglin_bis+1\n",
    "print(nb_goodpred_reglin_bis/(len(predicted_reglin_bis)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704.663149911\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique sur la base de test\n",
    "e_reglin=((predicted_reglin_bis-expected_reglin_bis)**2).sum()\n",
    "print(e_reglin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : comparaison des prédictions aux qualités réelles\n",
    "\n",
    "plt.hexbin(expected_reglin_bis,predicted_reglin_bis, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-On n'observe pas de différence fondamentale de performance. (même taux de bonnes réponses et erreur quadratique similaire) \n",
    "\n",
    "-Néanmoins les coefficients obtenus sont très différents de précédemment, certains ont même un signe différent.\n",
    "\n",
    "-Il faudrait alors estimer la variance des coefficients de la droite de régression et regarder si elle est véritablement plus faible pour pouvoir choisir entre les deux modèles celui qui est le plus \"stable\" lorsqu'on change de base d'apprentissage.\n",
    "L'autre possibilité, qui est mise en place plus bas, est d'utiliser une régression Ridge pour de toute façon réduire la variance des coefficients quitte à introduire un biais. Ceci permet de ne pas s'occuper de la multicolinéarité liée à la variable pH mais aussi celle des autres variables (total sulfur dioxide et free sulfur dioxide par exemple)\n",
    "\n",
    "-Rien qu'en testant sur plusieurs bases de tests (en enlevant random state à l'étape train test split et en rééxécutant le programme plusieurs fois), on remarque que la performance ne change pas vraiment. Je n'ai donc pas creusé dans cette direction, j'ai plutôt essayé d'améliorer la performance avant la \"stabilité\" des prédictions sur des bases d'apprentissages différentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : En enlevant des vins de qualité moyenne: (Attention on n'opère ici plus du tout sur les mêmes bases train et text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai voulu regarder ici, puisque les vins de qualité extrêmes étaient très mal prédits, si en rééquilibrant les proportions des différents vins, la droite de régression ne s'ajustait pas différemment en \"prenant plus en compte\" les vins de qualité extrême."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1369, 13)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On crée une nouvelle variable taken qui prend au hasard l'un des entiers parmi 0, 1 et 2 lorsque le vin est de qualité moyenne. Lorsque le vin est de qualité extrême, taken prend nécessairement la valeur 0.\n",
    "#On ne garde ensuite que les vins qui ont pris la modalité 0 pour la variable taken. On supprime ainsi environ deux tiers des vins de qualité moyenne de la base.\n",
    "import random\n",
    "dfwhite_lisse=dfwhite.copy()\n",
    "\n",
    "\n",
    "dfwhite_lisse['taken']=dfwhite_lisse['quality'].apply(lambda x: 0 if (x<5|x>7) else random.randint(0,3))\n",
    "dfwhite_lisse=dfwhite_lisse[dfwhite_lisse['taken']==0]\n",
    "dfwhite_lisse.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#On réutilise train test split, ici on n'opère donc plus sur une base de test similaire : les vins de la nouvelle base de test ne seront pas nécessairement dans la base de test précédente.\n",
    "#Ce n'est pas très grave car l'idée est déjà de voir ici s'il y a une réelle différente de performance dans la prédiction des vins de qualité extrême.\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_lisse, X_test_lisse, y_train_lisse, y_test_lisse = train_test_split(dfwhite_lisse[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']],dfwhite_lisse.quality, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.45909085e-01,  -1.81684845e+00,   2.79947760e-01,\n",
       "          1.35410747e-01,   9.61365174e-01,   4.60806744e-03,\n",
       "          1.58178490e-04,  -2.52620494e+02,   1.58050712e+00,\n",
       "          7.75520304e-01,   1.92912671e-01]),\n",
       " 248.00141567425661,\n",
       " 'R^2=',\n",
       " 0.33187780050260518)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement du modèle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reglin_lisse = LinearRegression(normalize=True)\n",
    "\n",
    "reglin_lisse.fit(X_train_lisse, y_train_lisse)\n",
    "\n",
    "reglin_lisse.coef_, reglin_lisse.intercept_, \"R^2=\",reglin_lisse.score(X_train_lisse, y_train_lisse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_reglin_lisse=reglin_lisse.predict(X_test_lisse)\n",
    "expected_reglin_lisse=y_test_lisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45348837209302323\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_reglin_lisse=0\n",
    "for i in range(0,len(predicted_reglin_lisse)) :\n",
    "    if round(predicted_reglin_lisse[i])==expected_reglin_lisse[i] :\n",
    "        nb_goodpred_reglin_lisse=nb_goodpred_reglin_lisse+1\n",
    "print(nb_goodpred_reglin_lisse/(len(predicted_reglin_lisse)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : Comparaison des prédictions aux qualités réelles\n",
    "\n",
    "plt.hexbin(expected_reglin_lisse,predicted_reglin_lisse, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe : erreur absolue selon la qualité espérée\n",
    "e_reglin_plot=abs(predicted_reglin-expected_reglin)\n",
    "\n",
    "ppl.scatter(expected_reglin, e_reglin_plot, color='indianred')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur absolue')\n",
    "plt.show()\n",
    "\n",
    "#On remarque que les vins de qualité extrême sont toujours mal prédits..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une distribution plus équitable des vins n'améliorent pas la performance, au contraire cela la dégrade légèrement car les vins de qualité moyenne sont moins bien prédits. J'interprète ce résultat comme le fait que les vins de qualité extrême présents dans la base semblent ne pas avoir suffisamment de caractéristiques communes, donc ou bien il n'y en a pas assez dans la base de données et donc le modèle ne réussit pas à les déterminer ou bien la notation à l'extrême est trop subjective pour être prédite ou bien il manque des informations (des variables) qui expliqueraient une note extrême."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression Ridge (Chapitre 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : Régression Ridge (linéaire) + Cross-validation pour trouver l'hyperparamètre alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.85997187e-02,  -1.91019639e+00,   3.55084364e-02,\n",
       "          6.67786859e-02,  -3.21307060e-01,   4.05924028e-03,\n",
       "         -7.22180739e-04,  -1.09090542e+02,   5.81707383e-01,\n",
       "          5.95690521e-01,   2.32119094e-01]),\n",
       " 109.5251476707829,\n",
       " 'R^2=',\n",
       " 0.28142497872855043,\n",
       " 0.01)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement du modèle Ridge linéaire avec validation croisée\n",
    "from sklearn import linear_model\n",
    "ridgereg = linear_model.RidgeCV(alphas=[0.1, 0.01,0.00001], normalize=True)\n",
    "\n",
    "ridgereg.fit(X_train, y_train)\n",
    "\n",
    "ridgereg.coef_, ridgereg.intercept_ , \"R^2=\",ridgereg.score(X_train, y_train), ridgereg.alpha_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction à partir de la base de test\n",
    "predicted_ridgereg = ridgereg.predict(X_test)\n",
    "expected_ridgereg = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5203915171288744\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_ridgereg=0\n",
    "for i in range(0,len(predicted_ridgereg)) :\n",
    "    if round(predicted_ridgereg[i])==expected_ridgereg[i] :\n",
    "        nb_goodpred_ridgereg=nb_goodpred_ridgereg+1\n",
    "print(nb_goodpred_ridgereg/(len(predicted_ridgereg)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701.451787903\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique\n",
    "e_ridgereg2=((predicted_ridgereg-expected_ridgereg)**2).sum()\n",
    "print(e_ridgereg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718.77317785\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue\n",
    "e_ridgereg1=abs(predicted_ridgereg-expected_ridgereg).sum()\n",
    "print(e_ridgereg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556.128634637\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_ridgereg_epsilon1=sum(abs(predicted_ridgereg[i]-expected_ridgereg[i]) for i in range(0, len(y_test)) if abs(predicted_ridgereg[i]-expected_ridgereg[i])>0.5)\n",
    "print(e_ridgereg_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.968594452\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 1\n",
    "e_ridgereg_epsilon1=sum(abs(predicted_ridgereg[i]-expected_ridgereg[i]) for i in range(0, len(y_test)) if abs(predicted_ridgereg[i]-expected_ridgereg[i])>1)\n",
    "print(e_ridgereg_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : Comparaison des prédictions aux qualités réelles\n",
    "plt.hexbin(expected_ridgereg,predicted_ridgereg, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.5  5.6  6.  ...,  5.6  5.7  5.4]\n"
     ]
    }
   ],
   "source": [
    "#Introduction de bruit sur les qualités \"vraies\" pour mieux voir les graphes et arrondi des préditions à un chiffre après la virgule pour des couleurs plus pertinentes\n",
    "expected_ridgereg_noise=expected_ridgereg+np.random.uniform(0,0.2,size=len(y_test))\n",
    "\n",
    "\n",
    "predicted_ridgereg_simplified=np.around(predicted_ridgereg,1)\n",
    "print(predicted_ridgereg_simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur absolue selon la qualité espérée\n",
    "e_ridgereg_plot=abs(predicted_ridgereg-expected_ridgereg)\n",
    "\n",
    "ppl.scatter(expected_ridgereg_noise, e_ridgereg_plot)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : Comparaison régresssion linéaire et régression Ridge (linéaire) => AUCUNE DIFFERENCE MAJEURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe : comparaison des 2 prédictions\n",
    "plt.scatter(predicted_reglin, predicted_ridgereg)\n",
    "plt.plot([3,9],[3,9],'--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Prédiction régression linéaire')\n",
    "plt.ylabel('Prédiction régression Ridge linéaire')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe : erreur absolue selon la qualité espérée\n",
    "e_ridgereg_plot=abs(predicted_ridgereg-expected_ridgereg)\n",
    "e_reglin_plot=abs(predicted_reglin-expected_reglin)\n",
    "\n",
    "ppl.scatter(expected_ridgereg_noise, e_ridgereg_plot)\n",
    "ppl.scatter(expected_reglin_noise, e_reglin_plot)\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme les différences ne sont pas très grandes en termes de performance et surtout de prédictions, même si normalement la régression Ridge devrait avoir des variances de coefficients moins élevés (ce que je n'ai pas vérifié ici), je n'ai pas développé ce modèle dans le rapport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation de X par une fonction de base polynomiale (degré 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3673, 22), (3673, 11), (1225, 22), (1225, 11))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On peut ici changer le degré facilement en changeant les nombres parcouru par j dans la boucle for. J'ai sélectionné 2 car les performances se dégradent quand on ajoute le degré 3.\n",
    "X_train_poly=X_train.copy()\n",
    "X_test_poly=X_test.copy()\n",
    "\n",
    "for j in range(2,3):\n",
    "    X_train_deg=X_train.copy()\n",
    "    X_test_deg=X_test.copy()\n",
    "    for i in range(0,11):\n",
    "        X_train_deg[:, i]=X_train_deg[:, i]**j\n",
    "        X_test_deg[:, i]=X_test_deg[:, i]**j\n",
    "    X_train_poly=np.concatenate((X_train_poly,X_train_deg), axis=1)\n",
    "    X_test_poly=np.concatenate((X_test_poly,X_test_deg), axis=1)\n",
    "    \n",
    "X_train_poly.shape, X_train.shape, X_test_poly.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression Ridge polynomiale de degré 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=[0.01, 0.1, 0.2, 0.3], cv=None, fit_intercept=True,\n",
       "    gcv_mode=None, loss_func=None, normalize=True, score_func=None,\n",
       "    scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement du modèle sur la base d'apprentissage avec validation croisée en même temps. \n",
    "#NB : Je n'ai laissé que ces alpha à tester après d'autres essais avec des alphas supérieurs qui n'étaient pas choisis pour que ça ne prenne pas trop de temps à réexécuter\n",
    "from sklearn import linear_model\n",
    "ridgereg_poly = linear_model.RidgeCV(alphas=[0.01, 0.1, 0.2, 0.3], normalize=True)\n",
    "\n",
    "ridgereg_poly.fit(X_train_poly, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.18322137e-02,  -1.34587489e+00,   4.19822691e-01,\n",
       "          3.13731056e-02,  -1.82919238e+00,   1.00351748e-02,\n",
       "          9.55537480e-04,  -2.68094383e+01,   1.67062638e-01,\n",
       "          2.64614983e-01,   1.19984375e-01,  -2.19378166e-03,\n",
       "         -5.48978962e-01,  -5.05219848e-01,   3.65203150e-04,\n",
       "          4.17705606e+00,  -6.18534057e-05,  -6.70087308e-06,\n",
       "         -1.33375828e+01,   2.94131388e-02,   2.16788094e-01,\n",
       "          6.62018958e-03]),\n",
       " 42.580267788047962,\n",
       " 'R^2=',\n",
       " 0.2968238780703979,\n",
       " 0.10000000000000001)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridgereg_poly.coef_, ridgereg_poly.intercept_ , \"R^2=\",ridgereg_poly.score(X_train_poly, y_train), ridgereg_poly.alpha_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédictions sur la base de test\n",
    "predicted_ridgereg_poly = ridgereg_poly.predict(X_test_poly)\n",
    "expected_ridgereg_poly = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe scatter : comparaison prédictions aux qualités réelles\n",
    "ppl.scatter(expected_ridgereg_poly, predicted_ridgereg_poly, color='indianred')\n",
    "ppl.plot([3,9],[3,9],'--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Qualité prédite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5244698205546493\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_ridgereg_poly=0\n",
    "for i in range(0,len(predicted_ridgereg_poly)) :\n",
    "    if round(predicted_ridgereg_poly[i])==expected_ridgereg_poly[i] :\n",
    "        nb_goodpred_ridgereg_poly=nb_goodpred_ridgereg_poly+1\n",
    "print(nb_goodpred_ridgereg_poly/(len(predicted_ridgereg_poly)+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690.52275081\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique totale(utilisée dans l'apprentissage)\n",
    "e_ridgereg_poly2=((predicted_ridgereg_poly-expected_ridgereg_poly)**2).sum()\n",
    "print(e_ridgereg_poly2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713.164555609\n",
      "0.582175147436\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue totale\n",
    "e_ridgereg_poly1=abs(predicted_ridgereg_poly-expected_ridgereg_poly).sum()\n",
    "print(e_ridgereg_poly1)\n",
    "#Erreur absolue moyenne\n",
    "print(e_ridgereg_poly1/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549.246972914\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_ridgereg_poly_epsilon1=sum(abs(predicted_ridgereg_poly[i]-expected_ridgereg_poly[i]) for i in range(0, len(y_test)) if abs(predicted_ridgereg_poly[i]-expected_ridgereg_poly[i])>0.5)\n",
    "print(e_ridgereg_poly_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272.663199003\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = \n",
    "e_ridgereg_poly_epsilon2=sum(abs(predicted_ridgereg_poly[i]-expected_ridgereg_poly[i]) for i in range(0, len(y_test)) if abs(predicted_ridgereg_poly[i]-expected_ridgereg_poly[i])>1)\n",
    "print(e_ridgereg_poly_epsilon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Introduction de bruit sur les qualités \"vraies\" pour mieux voir les graphes et arrondi des préditions à un chiffre après la virgule pour des couleurs plus pertinentes\n",
    "expected_ridgereg_poly_noise=expected_ridgereg_poly+np.random.uniform(0,0.2,size=len(y_test))\n",
    "\n",
    "\n",
    "predicted_ridgereg_poly_simplified=np.around(predicted_ridgereg_poly,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : Comparaison des prédictions aux qualités réelles\n",
    "plt.hexbin(expected_ridgereg_poly_noise,predicted_ridgereg_poly_simplified, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "plt.plot([4.5,7.1],[4.5,7.1],'--k')\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction selon le taux d'alcool : les vins à taux d'alcool plus élevé sont prédits avec une qualité supérieure\n",
    "plt.scatter(expected_ridgereg_poly_noise, predicted_ridgereg_poly_simplified, c=X_test[:,10])\n",
    "plt.plot([3,9],[3,9],'--k')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Alcohol')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Qualité prédite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe selon caractéristique chimique (à décliner selon la variable voulue)\n",
    "ppl.scatter(x=X_test[:,3],y=abs(expected_ridgereg_poly-predicted_ridgereg_poly), color='lightslategray')\n",
    "plt.axis('tight')\n",
    "plt.title('Residual sugar')\n",
    "plt.xlabel('Residual sugar')\n",
    "plt.ylabel('Erreur absolue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphique : Erreur absolue selon caractéristique chimique (à décliner selon la variable voulue)\n",
    "#Avec les couleurs, on voit ainsi mieux si les pics d'erreurs sur les graphiques précédents ne sont pas tout simplement dûs au fait qu'il y a plus de vins présentant des valeurs similaires pour la variable considérée et donc il y a plus de \"chances\" que les vins tres mal prédits présentent aussi cette valeur pour la variable considérée \n",
    "ppl.scatter(x=arange(0,len(y_test)),y=abs(expected_ridgereg_poly-predicted_ridgereg_poly), c=X_test[:,10])\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.title('Alcohol')\n",
    "plt.xlabel('Vins blans')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison régression linéaire et régression ridge polynomiale de degré 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comparaison des prédictions régression linéaire simple et régression Ridge poly de degré 2\n",
    "plt.scatter(predicted_reglin, predicted_ridgereg_poly)\n",
    "plt.plot([3,9],[3,9],'--k')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Prédiction régression linéaire')\n",
    "plt.ylabel('Prédiction régression Ridge polynomiale')\n",
    "plt.show()\n",
    "#Il y a déjà plus de différences qu'entre la régression linéaire simple et la régression Ridge linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe : erreur absolue selon la qualité espérée\n",
    "e_ridgereg_poly_plot=abs(predicted_ridgereg_poly-expected_ridgereg_poly)\n",
    "e_reglin_plot=abs(predicted_reglin-expected_reglin)\n",
    "\n",
    "ppl.scatter(expected_ridgereg_poly_noise, e_ridgereg_poly_plot, color='green', label='Régression Ridge polynomiale')\n",
    "ppl.scatter(expected_reglin_noise, e_reglin_plot,color='orange', label='Régression linéaire classique')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe taux d'erreur - évolution\n",
    "e_sorted_reglin=sorted(e_reglin_plot)\n",
    "e_sorted_ridgereg_poly=sorted(e_ridgereg_poly_plot)\n",
    "\n",
    "ppl.plot(np.arange(0,len(y_test)), e_sorted_reglin, color='orange')\n",
    "ppl.plot(np.arange(0,len(y_test)), e_sorted_ridgereg_poly, color='green')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Erreur')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : Transformation de X par une fonction de base gaussienne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci n'est pas détaillé dans le rapport car ça ne fait pas vraiment changer la performance par rapport à la régression linéaire simple. Elle semble même se dégrader comme l'indique le taux de bonnes réponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_gauss=X_train.copy()\n",
    "X_test_gauss=X_test.copy()\n",
    "\n",
    "for i in range(0,11):\n",
    "    X_train_gauss[:, i]=exp(-((X_train_gauss[:, i]-mean(X_train_gauss[:, i]))**2)/(2*var(X_train_gauss[:, i])))\n",
    "    X_test_gauss[:, i]=exp(-((X_test_gauss[:, i]-mean(X_test_gauss[:, i]))**2)/(2*var(X_test_gauss[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.07245023, -0.06919286,  0.70975267,  0.53486636,  0.11265204,\n",
       "         0.49337973,  0.4118309 , -0.49470413, -0.20001275, -0.28571773,\n",
       "        -0.42408889]), 5.194219212727444, 'R^2=', 0.17061109394577323, 0.01)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn import linear_model\n",
    "ridgereg_gauss = linear_model.RidgeCV(alphas=[0.01, 0.1, 0.5, 1,2,3,4,5,6,7,8,9], normalize=True)\n",
    "\n",
    "ridgereg_gauss.fit(X_train_gauss, y_train)\n",
    "\n",
    "ridgereg_gauss.coef_, ridgereg_gauss.intercept_ , \"R^2=\",ridgereg_gauss.score(X_train_gauss, y_train), ridgereg_gauss.alpha_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_ridgereg_gauss = ridgereg_gauss.predict(X_test_gauss)\n",
    "expected_ridgereg_gauss = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4698205546492659\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_ridgereg_gauss=0\n",
    "for i in range(0,len(predicted_ridgereg_gauss)) :\n",
    "    if round(predicted_ridgereg_gauss[i])==expected_ridgereg_gauss[i] :\n",
    "        nb_goodpred_ridgereg_gauss=nb_goodpred_ridgereg_gauss+1\n",
    "print(nb_goodpred_ridgereg_gauss/(len(predicted_ridgereg_gauss)+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : Bagging sur la régression polynomiale de degré 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le principe du bagging est :\n",
    "- On génére  plusieurs échantillons bootstrap à partir de la base d'apprentissage (tirages avec remise à partir de la base d'apprentissage initiale où chaque vin a la même probabibilité d'être tiré) - avec la commande de scikit learn par défaut on créée 10 échantillons bootstrap\n",
    "\n",
    "- A partir de ces nouveaux échantillons, on entraîne pour chaque nouvel échantillon un modèle Ridge avec alpha=0.1 (hyperparamètre trouvé précédemment)\n",
    "\n",
    "- On prédit les qualités des vins de la base de test avec chacun de ces modèles (par défaut on a donc 10 prédictions par vin de la base de test)\n",
    "\n",
    "- La prédiction finale est une moyenne de toutes ces prédictions\n",
    "\n",
    "Objectifs : éviter le surapprentissage, éviter la variance des prédictions, on espère ainsi obtenir de meilleures prédictions\n",
    "\n",
    "Néanmoins, ce n'est pas vraiment le cas ici, je n'ai donc pas détaillé cet essai dans le rapport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=Ridge(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=True, solver='auto', tol=0.001),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=1, oob_score=False,\n",
       "         random_state=99, verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn import linear_model\n",
    "\n",
    "bagging = BaggingRegressor(linear_model.Ridge(alpha=0.1, normalize=True),random_state=99)\n",
    "#On utilise le alpha obtenu par validation croisée précédemment\n",
    "\n",
    "bagging.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2961443595305554"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging.score(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_bagging = bagging.predict(X_test_poly)\n",
    "expected_bagging = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5236541598694943\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_bagging=0\n",
    "for i in range(0,len(predicted_bagging)) :\n",
    "    if round(predicted_bagging[i])==expected_bagging[i] :\n",
    "        nb_goodpred_bagging=nb_goodpred_bagging+1\n",
    "print(nb_goodpred_bagging/(len(predicted_bagging)+1))\n",
    "#Comme prévu avec le R2 aucune amélioration des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690.701724707\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique\n",
    "e_bagging=((predicted_bagging-expected_bagging)**2).sum()\n",
    "print(e_bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression à vecteurs de support (Chapitre 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANNEXE : Régression à vecteurs de support (SVR) linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle n'est pas détaillée dans le rapport car elle obtient une moins bonne performance que la SVR avec noyau gaussien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=2.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.5, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=16,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "#NB :  je n'ai pas fait de validation croisée pour choisir C et epsilon mais on voit rapidement que la performance est quasi identique à précédemment\n",
    "from sklearn import svm\n",
    "svrlin=svm.SVR(C=2.0, epsilon=0.5, kernel='linear', random_state=16)\n",
    "svrlin.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26624845716430845"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrlin.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_svrlin = svrlin.predict(X_test)\n",
    "expected_svrlin = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5203915171288744\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses: similaire au taux obtenu avec la régression Ridge polynomiale\n",
    "nb_goodpred_svrlin=0\n",
    "for i in range(0,len(predicted_svrlin)) :\n",
    "    if round(predicted_svrlin[i])==expected_svrlin[i] :\n",
    "        nb_goodpred_svrlin=nb_goodpred_svrlin+1\n",
    "print(nb_goodpred_svrlin/(len(predicted_svrlin)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709.517836111\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique \n",
    "e_svrlin=((predicted_svrlin-expected_svrlin)**2).sum()\n",
    "print(e_svrlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "724.193256203\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue\n",
    "e_svrlin=(abs(predicted_svrlin-expected_svrlin)).sum()\n",
    "print(e_svrlin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556.407834013\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_svrlin_epsilon1=sum(abs(predicted_svrlin[i]-expected_svrlin[i]) for i in range(0, len(y_test)) if abs(predicted_svrlin[i]-expected_svrlin[i])>0.5)\n",
    "print(e_svrlin_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281.633542928\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 1\n",
    "e_svrlin_epsilon2=sum(abs(predicted_svrlin[i]-expected_svrlin[i]) for i in range(0, len(y_test)) if abs(predicted_svrlin[i]-expected_svrlin[i])>1)\n",
    "print(e_svrlin_epsilon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : Comparaison des prédictions aux qualités réelles\n",
    "plt.hexbin(expected_svrlin,predicted_svrlin, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Introduction de bruit sur les qualités \"vraies\" pour mieux voir les graphes et arrondi des préditions à un chiffre après la virgule\n",
    "expected_svrlin_noise=expected_svrlin+np.random.uniform(0,0.2,size=len(y_test))\n",
    "\n",
    "predicted_svrlin_simplified=np.around(predicted_svrlin,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe du taux d'erreur selon la qualité espérée\n",
    "e_ridgereg_poly_plot=(predicted_ridgereg_poly-expected_ridgereg_poly)**2\n",
    "e_svrlin_plot=(predicted_svrlin-expected_svrlin)**2\n",
    "\n",
    "import prettyplotlib as ppl\n",
    "import matplotlib.pyplot as plt\n",
    "ppl.scatter(expected_svrlin_noise, e_svrlin_plot, color='green')\n",
    "ppl.scatter(expected_ridgereg_poly_noise, e_ridgereg_poly_plot,color='orange')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression à vecteurs de support avec noyau gaussien (parties 4.1 jusqu'à 4.6 incluse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=2.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=16,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn import svm\n",
    "svrrbf=svm.SVR(C=2.0, epsilon=0.3,kernel='rbf', random_state=16)\n",
    "svrrbf.fit(X_train,y_train)\n",
    "\n",
    "#NB : les hyperparamètres C et epsilon sont choisis par validation croisée ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81390272520296747"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrrbf.score(X_train, y_train)\n",
    "#On observe une augmentation du score en prenant 2 au lien de 1 (valeur par défaut) comme valeur de C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_svrrbf = svrrbf.predict(X_test)\n",
    "expected_svrrbf = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6076672104404568\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_svrrbf=0\n",
    "for i in range(0,len(predicted_svrrbf)) :\n",
    "    if round(predicted_svrrbf[i])==expected_svrrbf[i] :\n",
    "        nb_goodpred_svrrbf=nb_goodpred_svrrbf+1\n",
    "print(nb_goodpred_svrrbf/(len(predicted_svrrbf)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6581272084805654\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses sur les vins moyens (qualités 5, 6 et 7)\n",
    "\n",
    "nb_goodpred_svrrbf_commonwines=0\n",
    "for i in range(0,len(predicted_svrrbf)) :\n",
    "    if round(predicted_svrrbf[i])==expected_svrrbf[i] & 4<expected_svrrbf[i]<8 :\n",
    "        nb_goodpred_svrrbf_commonwines=nb_goodpred_svrrbf+1\n",
    "print(nb_goodpred_svrrbf/(list(y_test).count(5)+list(y_test).count(6)+list(y_test).count(7)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "703.545033248\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique totale\n",
    "e_svrrbf2=((predicted_svrrbf-expected_svrrbf)**2).sum()\n",
    "print(e_svrrbf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698.611861709\n",
      "0.570295397314\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue totale et moyenne\n",
    "e_svrrbf1=(abs(predicted_svrrbf-expected_svrrbf)).sum()\n",
    "print(e_svrrbf1)\n",
    "e_svrrbf1_mean = e_svrrbf1/len(y_test)\n",
    "print(e_svrrbf1_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495.810611618\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_svrrbf_epsilon1=sum(abs(predicted_svrrbf[i]-expected_svrrbf[i]) for i in range(0, len(y_test)) if abs(predicted_svrrbf[i]-expected_svrrbf[i])>0.5)\n",
    "print(e_svrrbf_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294.40939311\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 1\n",
    "e_svrrbf_epsilon2=sum(abs(predicted_svrrbf[i]-expected_svrrbf[i]) for i in range(0, len(y_test)) if abs(predicted_svrrbf[i]-expected_svrrbf[i])>1)\n",
    "print(e_svrrbf_epsilon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : Comparaison des prédictions aux qualités réelles\n",
    "plt.hexbin(expected_svrrbf,predicted_svrrbf, cmap=plt.cm.YlGn)\n",
    "#plt.axis([3, 9, 3, 9])\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.plot([4.3,7.7],[4.3,7.7],'--k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Introduction de bruit sur les qualités \"vraies\" pour mieux voir les graphes et arrondi des préditions à un chiffre après la virgule\n",
    "expected_svrrbf_noise=expected_svrrbf+np.random.uniform(0,0.2,size=len(y_test))\n",
    "\n",
    "predicted_svrrbf_simplified=np.around(predicted_svrrbf,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe scatter avec du bruit et arrondi à 0.1 près des prédictions (pour mieux voir les prédictions des qualités extrêmes)\n",
    "ppl.scatter(expected_svrrbf_noise,predicted_svrrbf_simplified)\n",
    "plt.axis([2.5, 9.5, 2.5, 9.5])\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "plt.plot([2.5,9.5],[2.5,9.5],'--k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur selon taux d'alcool : il semble que les vins avec un taux d'alcool élevé sont mal prédits\n",
    "ppl.scatter(x=X_test[:,10],y=abs(expected_svrrbf-predicted_svrrbf), color='indianred')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Alcohol')\n",
    "plt.ylabel('Erreur de prédiction')\n",
    "plt.title('Alcohol')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Distribution des taux d'alcool dans la base\n",
    "pandas.DataFrame.hist(dfwhite, column=\"alcohol\", bins=50, color='indianred', grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur selon densité : il semble que les vins avec une faible densité sont plus mal prédits\n",
    "ppl.scatter(x=X_test[:,7],y=abs(expected_svrrbf_noise-predicted_svrrbf_simplified), color='green')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Density')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.title('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur selon residual sugar : il semble que les vins avec peu de sucre sont moins bien prédits\n",
    "ppl.scatter(x=X_test[:,3],y=abs(expected_svrrbf_noise-predicted_svrrbf_simplified), color='lightslategray')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Residual sugar')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.title('Residual sugar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur à décliner selon les autres variables\n",
    "#Les autres variables donnent des résultats moins intéressants\n",
    "ppl.scatter(x=X_test[:,9],y=abs(expected_svrrbf_noise-predicted_svrrbf_simplified), color='blue')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Sulphates')\n",
    "plt.ylabel('Erreur en valeur absolue')\n",
    "plt.title('Sulphates')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction selon le taux d'alcool : les vins à taux d'alcool plus élevé sont prédits avec une qualité supérieure\n",
    "plt.scatter(expected_svrrbf_noise, predicted_svrrbf_simplified, c=X_test[:,10])\n",
    "plt.plot([3,9],[3,9],'--k')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Alcohol')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Qualité prédite')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison régression Ridge polynomiale et régression à vecteurs de suppport à noyau gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur absolue selon la qualité espérée\n",
    "\n",
    "#Introduction de plus de bruit sur la qualité réelle pour mieux voir\n",
    "expected_ridgereg_poly_noise2=expected_ridgereg_poly+np.random.uniform(0,0.5,size=len(y_test))\n",
    "expected_svrrbf_noise2=expected_svrrbf+np.random.uniform(0,0.5,size=len(y_test))\n",
    "\n",
    "e_ridgereg_poly_plot=abs(predicted_ridgereg_poly-expected_ridgereg_poly)\n",
    "e_svrrbf_plot=abs(predicted_svrrbf-expected_svrrbf)\n",
    "\n",
    "ppl.scatter(expected_svrrbf_noise2, e_svrrbf_plot, color='indianred', label='Régression à vecteurs de support')\n",
    "ppl.scatter(expected_ridgereg_poly_noise2, e_ridgereg_poly_plot,color='green', label='Régression Ridge polynomiale')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe taux d'erreur - évolution\n",
    "e_sorted_svrrbf=sorted(e_svrrbf_plot)\n",
    "e_sorted_ridgereg_poly=sorted(e_ridgereg_poly_plot)\n",
    "\n",
    "ppl.plot(np.arange(0,len(y_test)), e_sorted_svrrbf, color='green')\n",
    "ppl.plot(np.arange(0,len(y_test)), e_sorted_ridgereg_poly, color='orange')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Erreur')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search (selon critère de performance par défaut dans scikit learn) pour choisir les hyperparamètres epsilon et C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import grid_search\n",
    "\n",
    "grid = {'C': [0.5,1,2,5,10,50, 100], 'epsilon': [0.1,0.2,0.3,0.5,0.6,0.7,0.8,0.9,1,1.5]}\n",
    "gd=grid_search.GridSearchCV(estimator=svm.SVR(kernel='rbf'), param_grid=grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'epsilon': 0.3}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recherche de l'hyperparamètre C par validation croisée 5-Fold en essayant de minimiser les erreurs supérieures à 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 423.98099214  332.06285514  327.78732341  323.30129518  328.21594039\n",
      "   334.02911909  342.14213022  355.63336055  355.64661697]\n",
      " [ 429.65640689  351.90781643  343.89362992  348.10773268  351.17542989\n",
      "   353.38733513  363.0016949   371.65253798  371.67535989]\n",
      " [ 397.26808553  302.49605823  304.24697251  301.90533789  306.30772221\n",
      "   308.32944547  319.92945651  342.6932866   342.70713224]\n",
      " [ 414.12409568  328.21096399  326.47314697  329.59199857  328.67359699\n",
      "   328.34073722  334.21936203  344.04131358  344.04131358]\n",
      " [ 435.26505291  323.297916    322.96112763  328.97374987  331.15149946\n",
      "   336.0522446   348.92226039  365.82274723  365.87065835]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "kf = cross_validation.KFold(len(y_train), n_folds=5)\n",
    "\n",
    "hyperparam=[0.1,1.0,2.0,3.0,4.0,5.0,10.0,100,1000]\n",
    "nb_iter=-1\n",
    "mat_error=np.zeros((5,len(hyperparam)))\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    nb_iter=nb_iter+1\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_v, X_valid = X_train[train_index], X_train[test_index]\n",
    "    y_train_v, y_valid = y_train[train_index], y_train[test_index]\n",
    "    #On entraîne le modèle pour toutes les valeurs d'hyperparamètres sur chaque base d'apprentissage et on prédit les qualités sur chaque base de validation\n",
    "    for j in range(0,len(hyperparam)):\n",
    "        svr_bis=svm.SVR(C=hyperparam[j],kernel='rbf', epsilon=0.1)\n",
    "        svr_bis.fit(X_train_v,y_train_v)\n",
    "        predicted_svr_bis=svr_bis.predict(X_valid)\n",
    "        expected_svr_bis=y_valid\n",
    "        e_svr_bis=abs(predicted_svr_bis-expected_svr_bis)\n",
    "        #On ne décompte que les erreurs absolues supérieures à 0.5 : (les autres comptent pour 0)\n",
    "        for k in range(0,len(e_svr_bis)):\n",
    "            if e_svr_bis[k]<0.5:\n",
    "                e_svr_bis[k]=0\n",
    "        mat_error[nb_iter,j]=e_svr_bis.sum()\n",
    "\n",
    "print(mat_error)\n",
    "#On obtient une matrice avec en colonnes les différentes valeurs de C et en ligne les différents couples (base d'apprentissage, base de validation)\n",
    "#Il s'agit donc d'une matrice à 9 colonnes (nombre d'hyperparamètres C testés) et 5 lignes (nombre de couples, car c'est une 5-Fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2100.29463315  1637.97560978  1625.36220044  1631.88011419\n",
      "   1645.52418894  1660.13888151  1708.21490405  1779.84324594\n",
      "   1779.94108101]]\n"
     ]
    }
   ],
   "source": [
    "#Pour chaque valeur de C, on somme les erreurs obtenues sur chacune des 5 bases de validation\n",
    "mean_error=np.zeros((1,len(hyperparam)))\n",
    "for i in range (0,len(hyperparam)):\n",
    "    mean_error[0,i]=mat_error[:,i].sum()\n",
    "print(mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#On choisit l'hyperparamètre C qui a la somme des erreurs sur chacune des bases de validation la plus petite (ceci est équivalent à choisir celui qui a la moyenne des erreurs sur les 5 bases de test la plus petite)\n",
    "best_param=0\n",
    "for i in range (0,len(hyperparam)):\n",
    "    if mean_error[0,i]==min(mean_error[0,:]):\n",
    "        best_param=hyperparam[i]\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En minimisant les erreurs supérieures à 0.5 :On trouve C=2 pour epsilon=0.1 (avec erreur moyenne sur toutes les bases de test = 1625.3)\n",
    "- En minimisant les erreurs supérieures à 0.5 :On trouve C=2 pour epsilon=0.3 (avec erreur moyenne sur toutes les bases de test= 1558.3)\n",
    "- En minimisant les erreurs supérieures à 1 :On trouve C=2 pour epsilon=0.3\n",
    "- En minimisant les erreurs supérieures à 1 :On trouve C=1 pour epsilon=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double modèle selon le taux d'alcool du vin dont la qualité est à prédire (Chapitre 4, partie 4.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Division de la base initiale en une base qui contient les vins à plus de 12 degrés d'alcool et une base qui contient les vins à moins de 12 degrés d'alcool)\n",
    "df_high=dfwhite[dfwhite.alcohol>12]\n",
    "df_low=dfwhite[dfwhite.alcohol<=12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((711, 12), (4187, 12))"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_high.shape, df_low.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Entraînement sur chacune des 2 nouvelles bases d'apprentissage\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_high, X_test_high, y_train_high, y_test_high = train_test_split(df_high[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']],df_high.quality, random_state=3)\n",
    "\n",
    "X_train_low, X_test_low, y_train_low, y_test_low = train_test_split(df_low[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']],df_low.quality, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Création d'une nouvelle \"grande\" base d'apprentissage et \"grande\" base de test contenant des vins à taux d'alcool quelconque pour comparer ensuite les résultats sur la même base de test\n",
    "new_X_train =np.concatenate((X_train_low,X_train_high), axis=0)\n",
    "new_X_test=np.concatenate((X_test_low,X_test_high), axis=0)\n",
    "new_y_train=np.concatenate((y_train_low,y_train_high), axis=0)\n",
    "new_y_test=np.concatenate((y_test_low,y_test_high), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sur la nouvelle \"grande\" base de test et apprentissage (uniquement pour comparer les résultats, on doit normalement retrouver des résultats proches de ci-dessus) : entraînement d'un modèle SVR à noyau gaussien et prédiction sur la nouvelle grande base de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=2.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.3, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=16,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn import svm\n",
    "svrrbf=svm.SVR(C=2.0, epsilon=0.3,kernel='rbf', random_state=16)\n",
    "svrrbf.fit(new_X_train,new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82316858879938648"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrrbf.score(new_X_train, new_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_svrrbf_new = svrrbf.predict(new_X_test)\n",
    "expected_svrrbf_new = new_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5864600326264274\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "nb_goodpred_svrrbf_new=0\n",
    "for i in range(0,len(predicted_svrrbf_new)) :\n",
    "    if round(predicted_svrrbf_new[i])==expected_svrrbf_new[i] :\n",
    "        nb_goodpred_svrrbf_new=nb_goodpred_svrrbf_new+1\n",
    "print(nb_goodpred_svrrbf_new/(len(predicted_svrrbf_new)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "709.581447477\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique totale\n",
    "e_svrrbf2_new=((predicted_svrrbf_new-expected_svrrbf_new)**2).sum()\n",
    "print(e_svrrbf2_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711.198049746\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue totale\n",
    "e_svrrbf1_new=(abs(predicted_svrrbf_new-expected_svrrbf_new)).sum()\n",
    "print(e_svrrbf1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514.557036435\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_svrrbf_epsilon1_new=sum(abs(predicted_svrrbf_new[i]-expected_svrrbf_new[i]) for i in range(0, len(new_y_test)) if abs(predicted_svrrbf_new[i]-expected_svrrbf_new[i])>0.5)\n",
    "print(e_svrrbf_epsilon1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295.079647656\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 1\n",
    "e_svrrbf_epsilon2_new=sum(abs(predicted_svrrbf_new[i]-expected_svrrbf_new[i]) for i in range(0, len(new_y_test)) if abs(predicted_svrrbf_new[i]-expected_svrrbf_new[i])>1)\n",
    "print(e_svrrbf_epsilon2_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sur la base low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=2.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=16,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn import svm\n",
    "svrrbf_low=svm.SVR(C=2.0, epsilon=0.2,kernel='rbf', random_state=16)\n",
    "#Les hyperparamètres sont choisis selon la validation croisée effectuée plus bas\n",
    "svrrbf_low.fit(X_train_low,y_train_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85975390843562893"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrrbf_low.score(X_train_low,y_train_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_low=svrrbf_low.predict(X_test_low)\n",
    "expected_low=y_test_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n"
     ]
    }
   ],
   "source": [
    "#Nombre de bonnes réponses sur la base de test low\n",
    "nb_goodpred_low=0\n",
    "for i in range(0,len(predicted_low)) :\n",
    "    if round(predicted_low[i])==expected_low[i] :\n",
    "        nb_goodpred_low=nb_goodpred_low+1\n",
    "print(nb_goodpred_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sur la base high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.5, cache_size=200, coef0=0.0, degree=3, epsilon=0.5, gamma=0.0,\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=16,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entraînement\n",
    "from sklearn import svm\n",
    "\n",
    "svrrbf_high=svm.SVR(C=0.5, epsilon=0.5,kernel='rbf', random_state=16)\n",
    "#Les hyperparamètres sont choisis selon la validation croisée effectuée plus bas\n",
    "\n",
    "svrrbf_high.fit(X_train_high,y_train_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36213979481467407"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svrrbf_high.score(X_train_high,y_train_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prédiction\n",
    "predicted_high=svrrbf_high.predict(X_test_high)\n",
    "expected_high=y_test_high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "#Nombre de bonnes réponses sur la base high\n",
    "nb_goodpred_high=0\n",
    "for i in range(0,len(predicted_high)) :\n",
    "    if round(predicted_high[i])==expected_high[i] :\n",
    "        nb_goodpred_high=nb_goodpred_high+1\n",
    "print(nb_goodpred_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe hexbin : comparaison des prédictions aux qualités réelles\n",
    "#On n'observe déjà qu'il n'y a pas vraiment d'amélioration\n",
    "plt.hexbin(expected_high,predicted_high, cmap=plt.cm.YlGn)\n",
    "plt.title(\"Comparaison prédiction vs qualité réelle\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('Nombre de prédictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erreurs totales (sur base de test low + base de test high = \"grande\" base de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5755102040816327\n"
     ]
    }
   ],
   "source": [
    "#Total de bonnes prédictions sur la \"grande\" base de test avec le double modèle\n",
    "nb_goodpred_total=(nb_goodpred_high+nb_goodpred_low)/(len(y_test_high)+len(y_test_low))\n",
    "print(nb_goodpred_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682.727607796\n"
     ]
    }
   ],
   "source": [
    "#Erreur absolue totale sur la \"grande\" base de test avec le double modèle\n",
    "e_total1=(abs(predicted_high-expected_high)).sum()+(abs(predicted_low-expected_low)).sum()\n",
    "print(e_total1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "671.526759722\n"
     ]
    }
   ],
   "source": [
    "#Erreur quadratique totale sur la \"grande\" base de test avec le double modèle\n",
    "e_total2=((predicted_high-expected_high)**2).sum()+((predicted_low-expected_low)**2).sum()\n",
    "print(e_total2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504.531002513\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 0.5\n",
    "e_high_epsilon1=sum(abs(predicted_high[i]-expected_high[i]) for i in range(0, len(y_test_high)) if abs(predicted_high[i]-expected_high[i])>0.5)\n",
    "e_low_epsilon1=sum(abs(predicted_low[i]-expected_low[i]) for i in range(0, len(y_test_low)) if abs(predicted_low[i]-expected_low[i])>0.5)\n",
    "e_total_epsilon1=e_high_epsilon1+e_low_epsilon1\n",
    "print(e_total_epsilon1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.646865408\n"
     ]
    }
   ],
   "source": [
    "#Erreur sans prendre en compte les erreurs en valeur absolue inférieures à epsilon = 1\n",
    "e_high_epsilon2=sum(abs(predicted_high[i]-expected_high[i]) for i in range(0, len(y_test_high)) if abs(predicted_high[i]-expected_high[i])>1)\n",
    "e_low_epsilon2=sum(abs(predicted_low[i]-expected_low[i]) for i in range(0, len(y_test_low)) if abs(predicted_low[i]-expected_low[i])>1)\n",
    "e_total_epsilon2=e_high_epsilon2+e_low_epsilon2\n",
    "print(e_total_epsilon2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gridsearch pour trouver les hyperparamètres sur les bases low et high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import grid_search\n",
    "\n",
    "grid = {'C': [0.5,1,2,5,10,50, 100], 'epsilon': [0.1,0.2,0.3,0.5,0.6,0.7,0.8,0.9,1,1.5]}\n",
    "gd=grid_search.GridSearchCV(estimator=svm.SVR(kernel='rbf'), param_grid=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.5, 'epsilon': 0.5}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.fit(X_train_high,y_train_high)\n",
    "gd.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2, 'epsilon': 0.2}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.fit(X_train_low,y_train_low)\n",
    "gd.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparaison entre le modèle SVR simple et le double modèle selon le taux d'alcool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Graphe erreur absolue selon la qualité espérée\n",
    "\n",
    "expected_svrrbf_new_noise=expected_svrrbf_new+np.random.uniform(0,0.5,size=len(new_y_test))\n",
    "expected_low_noise=expected_low+np.random.uniform(0,0.5,size=len(y_test_low))\n",
    "expected_high_noise=expected_high+np.random.uniform(0,0.5,size=len(y_test_high))\n",
    "\n",
    "\n",
    "import prettyplotlib as ppl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "ppl.scatter(expected_svrrbf_new_noise, abs(expected_svrrbf_new-predicted_svrrbf_new), color='indianred', label='SVR sur base complète')\n",
    "ppl.scatter(expected_low_noise, abs(expected_low-predicted_low),color='green', label='SVR sur base de vins à faible taux dalcool')\n",
    "ppl.scatter(expected_high_noise, abs(expected_high-predicted_high),color='green', label='SVR sur base de vins à fort taux dalcool')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Vraie qualité')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEXE : Problème de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai commencé à réfléchir ici à un éventuel moyen de détecter les mauvais vins par classification binaire. J'ai considéré les vins de qualités 3 et 4 comme ds \"mauvais\" vin (sachant qu'il n'y a pas de qualités strictement inférieures à 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transformation de la variable quality en une variable binaire indiquant si le vin est mauvais ou pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwhite_2=dfwhite.copy()\n",
    "dfwhite_2['poor wine']=dfwhite_2['quality'].apply(lambda x: 1 if x<5 else -1)\n",
    "dfwhite_2=dfwhite_2.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>poor wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td> 6.2</td>\n",
       "      <td> 0.21</td>\n",
       "      <td> 0.29</td>\n",
       "      <td> 1.6</td>\n",
       "      <td> 0.039</td>\n",
       "      <td> 24</td>\n",
       "      <td>  92</td>\n",
       "      <td> 0.99114</td>\n",
       "      <td> 3.27</td>\n",
       "      <td> 0.50</td>\n",
       "      <td> 11.2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td> 6.6</td>\n",
       "      <td> 0.32</td>\n",
       "      <td> 0.36</td>\n",
       "      <td> 8.0</td>\n",
       "      <td> 0.047</td>\n",
       "      <td> 57</td>\n",
       "      <td> 168</td>\n",
       "      <td> 0.99490</td>\n",
       "      <td> 3.15</td>\n",
       "      <td> 0.46</td>\n",
       "      <td>  9.6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td> 6.5</td>\n",
       "      <td> 0.24</td>\n",
       "      <td> 0.19</td>\n",
       "      <td> 1.2</td>\n",
       "      <td> 0.041</td>\n",
       "      <td> 30</td>\n",
       "      <td> 111</td>\n",
       "      <td> 0.99254</td>\n",
       "      <td> 2.99</td>\n",
       "      <td> 0.46</td>\n",
       "      <td>  9.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td> 5.5</td>\n",
       "      <td> 0.29</td>\n",
       "      <td> 0.30</td>\n",
       "      <td> 1.1</td>\n",
       "      <td> 0.022</td>\n",
       "      <td> 20</td>\n",
       "      <td> 110</td>\n",
       "      <td> 0.98869</td>\n",
       "      <td> 3.34</td>\n",
       "      <td> 0.38</td>\n",
       "      <td> 12.8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td> 6.0</td>\n",
       "      <td> 0.21</td>\n",
       "      <td> 0.38</td>\n",
       "      <td> 0.8</td>\n",
       "      <td> 0.020</td>\n",
       "      <td> 22</td>\n",
       "      <td>  98</td>\n",
       "      <td> 0.98941</td>\n",
       "      <td> 3.26</td>\n",
       "      <td> 0.32</td>\n",
       "      <td> 11.8</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "4893                   24                    92  0.99114  3.27       0.50   \n",
       "4894                   57                   168  0.99490  3.15       0.46   \n",
       "4895                   30                   111  0.99254  2.99       0.46   \n",
       "4896                   20                   110  0.98869  3.34       0.38   \n",
       "4897                   22                    98  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol  poor wine  \n",
       "4893     11.2         -1  \n",
       "4894      9.6         -1  \n",
       "4895      9.4         -1  \n",
       "4896     12.8         -1  \n",
       "4897     11.8         -1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwhite_2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    4715\n",
       " 1     183\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfwhite_2['poor wine'].value_counts()\n",
    "#On remarque qu'on a très peu de mauvais vins, les classes sont disproportionnées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Séparation entre une base de test (25%) et une base d'entraînement (75%) pour prédiction de la variable 'poor wine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(dfwhite_2[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']],dfwhite_2['poor wine'], random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse discriminante linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'anayse discriminante, si une classe est sous-représentée ce n'est pas un problème car on apprend à prédire une classe contre l'autre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA(n_components=None, priors=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import lda\n",
    "\n",
    "linearda=lda.LDA()\n",
    "\n",
    "linearda.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.84691131e+00,   2.74641542e-01,   3.35803344e-01,\n",
       "          6.51734202e+00,   4.59832814e-02,   3.55508643e+01,\n",
       "          1.38025928e+02,   9.94051547e-01,   3.18597620e+00,\n",
       "          4.90427883e-01,   1.05139265e+01],\n",
       "       [  7.26111111e+00,   3.81250000e-01,   3.09861111e-01,\n",
       "          4.45000000e+00,   5.01180556e-02,   2.57187500e+01,\n",
       "          1.27180556e+02,   9.94181806e-01,   3.17611111e+00,\n",
       "          4.83333333e-01,   1.02322917e+01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearda.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_linearda=linearda.predict(X_test_2)\n",
    "expected_linearda=y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9608482871125612\n"
     ]
    }
   ],
   "source": [
    "#Taux de bonnes réponses\n",
    "#Ca n'indique en fait ici pas grand chose puisque comme la classe des mauvais vins est sous-représentée, le classifier obtient un taux de bonnes réponses très élevée en ne prédisant que \"bon\" et jamais \"mauvais\"\n",
    "nb_goodpred_linearda=0\n",
    "for i in range(0,len(predicted_linearda)) :\n",
    "    if predicted_linearda[i]==expected_linearda[i]:\n",
    "        nb_goodpred_linearda=nb_goodpred_linearda+1\n",
    "print(nb_goodpred_linearda/(len(predicted_linearda)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "#Nombres de mauvais vins prédits \"mauvais\"\n",
    "nb_goodpred_linearda_poor=0\n",
    "for i in range(0,len(predicted_linearda)) :\n",
    "    if predicted_linearda[i]==expected_linearda[i] and expected_linearda[i]==1 :\n",
    "        nb_goodpred_linearda_poor=nb_goodpred_linearda_poor+1\n",
    "print(nb_goodpred_linearda_poor)\n",
    "#C'est bien ce que donne la matrice de confusion ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1172   14]\n",
      " [  33    6]]\n"
     ]
    }
   ],
   "source": [
    "#Matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_linearda = confusion_matrix(expected_linearda,predicted_linearda)\n",
    "print(cm_linearda)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm_linearda)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Nombre de \"vrais\" mauvais vins dans la base de test\n",
    "(y_test_2==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On a donc détecté ce pourcentage de mauvais vins:\n",
    "cm_linearda[1,1]/(cm_linearda[1,0]+cm_linearda[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il détecte mal les mauvais vins alors que c'est ce qui nous intéresse. Heureusement, ce classifier prédit peu de bons vins comme étant mauvais (14 selon la matrice de confusion). Ainsi, si l'on imagine un programme qui évite de goûter tous les vins, goûter seulement 14 + 6 vins est vraiment peu coûteux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Courbe ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : Dans un test de classification binaire, un résultat est dit vrai positif lorsqu'un item est correctement détecté par le test. On l'oppose aux notions de faux positif (item déclaré positif alors qu'il ne l'était pas), de faux négatif (item déclaré négatif alors qu'il était en réalité positif) et de vrai négatif (item correctement déclaré comme négatif).\n",
    "\n",
    "Dans le cas présent, idéalement on aimerait obtenir 0 faux négatif (aucun vin mauvais prédit \"bon\" c'est-à-dire prédit -1) et à ce moment là le nombre de vrai négatif est moins important car dans tous les cas on devra goûter moins de vins que si on les goûtait tous pour trouver les mauvais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_score = linearda.fit(X_train_2, y_train_2).decision_function(X_test_2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_2,y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "#fraction des Négatifs classés positifs\n",
    "plt.ylabel('True Positive Rate')\n",
    "#fraction des Positifs classés positifs\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus l'aire sous la courbe est grande, plus elle se rapproche du classifier idéal (au point (0,1))\n",
    "\n",
    "Remarque : On voit sur cette courbe ROC qu'en mettant fixant arbitrairement un certain seuil de prédiction, on pourrait détecter environ 90% des mauvais vins (true positive rate = 0.9 ) en goûtant beaucoup moins de vins que la totalité puisque le false positive rate est environ à 0.65 pour ce true positive rate. On serait donc obligés de goûter environ 65% des vins qui en réalité sont bons au lieu de 100%, ce qui entraînerait une forte diminution des coûts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem pas de problème de sous-représentation ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QDA(priors=None, reg_param=0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import qda\n",
    "\n",
    "quadrada=qda.QDA()\n",
    "\n",
    "quadrada.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_quadrada=quadrada.predict(X_test_2)\n",
    "expected_quadrada=y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1156   30]\n",
      " [  29   10]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_quadrada = confusion_matrix(expected_quadrada,predicted_quadrada)\n",
    "print(cm_quadrada)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm_quadrada)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25641025641025639"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On a donc détecté ce pourcentage de mauvais vins:\n",
    "cm_quadrada[1,1]/(cm_quadrada[1,0]+cm_quadrada[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a une légère amélioration par rapport à l'analyse discriminante linéaire : on détecte 1/4 des mauvais vins. Et on prédit 40 mauvais vins, c'est assez peu donc c'est plutôt un bon résultat. Mais l'air en-dessous de la courbe ROC est légèrement moins grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Courbe ROC\n",
    "from sklearn import metrics\n",
    "y_score = quadrada.fit(X_train_2, y_train_2).decision_function(X_test_2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_2,y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "#fraction des Négatifs classés positifs\n",
    "plt.ylabel('True Positive Rate')\n",
    "#fraction des Positifs classés positifs\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine à Vecteurs de support - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette méthode, il faudrait plus réfléchir au problème de sous-représentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SVM Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.5, cache_size=200, class_weight='auto', coef0=0.0, degree=3,\n",
       "  gamma=0.0, kernel='linear', max_iter=-1, probability=False,\n",
       "  random_state=None, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "#class_weight : {dict, ‘auto’}, optional\n",
    "#Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ‘auto’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies.\n",
    "\n",
    "#On donne plus de poids à la classe sous-représentée car c'est celle qui nous intéresse\n",
    "svm_lin=svm.SVC(C=0.5,kernel='linear', class_weight='auto')\n",
    "svm_lin.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_svmlin=svm_lin.predict(X_test_2)\n",
    "expected_svmlin=y_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[952 234]\n",
      " [ 18  21]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_svmlin = confusion_matrix(expected_svmlin,predicted_svmlin)\n",
    "print(cm_svmlin)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm_svmlin)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53846153846153844"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On a donc détecté ce pourcentage de mauvais vins:\n",
    "cm_svmlin[1,1]/(cm_svmlin[1,0]+cm_svmlin[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On détecte 50% de vrais mauvais vins sauf que on détecte 224 autres mauvais vins qui en réalité ne le sont pas, ce qui très coûteux comparé au nombre de vins total, surtout si parmi ces 224 il n'y a que la moitié des mauvais vins.\n",
    "\n",
    "On obtient une aire sous la courbe ROC légèrement moins grande que l'analyse discriminante linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Courbe ROC\n",
    "from sklearn import metrics\n",
    "y_score = svm_lin.fit(X_train_2, y_train_2).decision_function(X_test_2)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_2,y_score)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "#fraction des Négatifs classés positifs\n",
    "plt.ylabel('True Positive Rate')\n",
    "#fraction des Positifs classés positifs\n",
    "plt.title('ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-fold cross-validation sur l'hyperparamètre C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 177.  152.  148.  138.  139.  146.]\n",
      " [ 158.  122.  131.  149.  158.  159.]\n",
      " [ 168.  134.  132.  141.  148.  154.]\n",
      " [ 212.  200.  199.  202.  198.  198.]\n",
      " [ 199.  169.  176.  178.  181.  191.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "kf = cross_validation.KFold(len(y_train_2), n_folds=5)\n",
    "\n",
    "hyperparam=[0.1,0.5,1.0,3,10.0,20.0]\n",
    "nb_iter=-1\n",
    "mat_error=np.zeros((5,len(hyperparam)))\n",
    "\n",
    "for train_index, test_index in kf:\n",
    "    nb_iter=nb_iter+1\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_v, X_valid = X_train_2[train_index], X_train_2[test_index]\n",
    "    y_train_v, y_valid = y_train_2[train_index], y_train_2[test_index]\n",
    "    #On entraîne le modèle pour toutes les valeurs d'hyperparamètres sur la base d'apprentissage et la base de validation obtenues\n",
    "    for j in range(0,len(hyperparam)):\n",
    "        svm_lin1=svm.SVC(C=hyperparam[j],kernel='linear', class_weight='auto')\n",
    "        svm_lin1.fit(X_train_v,y_train_v)\n",
    "        predicted_svmlin1=svm_lin1.predict(X_valid)\n",
    "        expected_svmlin1=y_valid\n",
    "        cm_svmlin1 = confusion_matrix(expected_svmlin1,predicted_svmlin1)\n",
    "        mat_error[nb_iter,j]=cm_svmlin1[0,1]+cm_svmlin1[1,0]\n",
    "\n",
    "print(mat_error)\n",
    "\n",
    "#NB : algo brute force : on essaye toutes les possibilités une par une\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 914.  777.  786.  808.  824.  848.]]\n"
     ]
    }
   ],
   "source": [
    "mean_error=np.zeros((1,len(hyperparam)))\n",
    "for i in range (0,len(hyperparam)):\n",
    "    mean_error[0,i]=mat_error[:,i].sum()\n",
    "print(mean_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "best_param=0\n",
    "for i in range (0,len(hyperparam)):\n",
    "    if mean_error[0,i]==min(mean_error[0,:]):\n",
    "        best_param=hyperparam[i]\n",
    "print(best_param)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
